---
title: "w203_Lab3_group5_draft1: Joanna wang, Douglas(Zeliang) Xu"
output:
  html_document: default
  pdf_document: default
---
### Introduction
In this report, we will analyze the crime statistics for a selection of counties in North Carolina and purpose to find out some main factors of crime rate. After the initial analysis on the data, we will purpose our research question.We will then provide several policies or suggestions based on our research findings. 

### Part1: Looking at the data

This part is to look at the dataset in general to grow our understand of the data:

```{r }
library(car)
library(sandwich)
library(lmtest)
data <- read.csv(file="crime_v2.csv", header=TRUE, sep=",",na.strings=c("`","","NA"))
objects(data)
```
Here we have the summary of all the veriables in the data set, to spot any anormaly. 
```{r}
summary(data)
```

We noticed that there are 6 NAs in all the variables. To took a closer look at the last few rows of the date set to verify the NA entires.  
```{r}
tail(data,11)
```
Some of the things we see in the dataset and have lead us to some decisions: 
1. 6 NAs for all columns. We will remove those entries. 
2. Year is alwasy 87. We will take out the year column, because it does not help us with our crime rate analysis
3. "prbarr" max > 1. Probability should not be greater than 1. 
4. "prbconv" strange characters and blank spaces; also the probability is bigger than 1
5. taxpc, what is the unit, what does it mean? Outlier at 119. Is the unit %? 
6. pctmin80 data is too old
7. 15-23: different industry avg. wages
8. 24 mix: ratio of face-to-face crime
9. percentage young male (what is the age:15-24)

With the import method modified, we are able to address the missing values and the special character error in the $prbconv$ data column

### Data Cleaning
1. remove NA
```{r }
data <- na.omit(data)
summary(data)
```
2. take out year column as it does not mean anything in our analysis
```{r }
data_clean <- subset(data,select=-c(year))
objects(data_clean)
```
3. "prbarr" max > 1. Becuase this variable is supposed to represent the probability of arrest, the max should never exceed 1. 
```{r }
data_clean <- subset(data_clean,data_clean$prbarr <1)
hist(data_clean$prbarr, breaks=20,  main="prbarr", xlab="prbarr")
```

4. "prbconv" column contains strange characters and blank spaces; Also becuase this is the probability, it should not contain entries that are bigger than 1 <br>
__blank space and strange characters are taken care of in data import__
```{r }
data_clean <- subset(data_clean,data_clean$prbconv <1)
hist(data_clean$prbconv, breaks=20, main="prbconv", xlab="prbconv")
```

5. taxpc: There seems to be some anomaly that is very far apart from other data points, but we have no evidence to say whether the data has anomaly or not. It could be an error or it could just be that the county has high tax per capita
```{r }
hist(data_clean$taxpc, breaks=20, main="taxpc", xlab="taxpc")
```

6. over-paid service industry: we found that the maximum value of the avg. weekly wage of service industry is above 2000, which is way more than the other industries. Based on the background knowledge, we don't believe there should be significant difference between the service industry and other industries in terms of compensation difference, and the data point above 2000 should be an error in the data, and we decided to remove them

```{r }
data_clean <- subset(data_clean, data_clean$wser<1000)
summary(data_clean$wser)
```

### EDA
1. As the first part of the data analysis, we are taking a look at the distributions of the dependent variable:

```{r }
hist(data_clean$crmrte,breaks=50, main="crmrte", xlab="crmrte")
```

```{r }
hist(log(data_clean$crmrte),breaks=50, main="Log transform of crmrte", xlab="Log of crmrte")
```

2. As the second part of the EDA, we take an initial look at the simple correlation between each variable inside the dataframe to help us understand the general correlations betwen each varible and the dependent variable to help identify the explanatory variables of interest
```{r }
# correlation plot
library(corrplot)
corrplot(cor(data_clean[,sapply(data_clean,is.numeric)]),is.corr=T, method = "circle", type='upper',main = "Correlation Plot", tl.cex=0.8)
```

### Research Question
After the initial data cleaning and data analysis, we have defined our research question to be: how to reduce crime rate within North Carolina, and especially what are the most effective measures in reducing crime rate.

The question is asked to help the political campaign to propose effective strategies to reduce crime rate, especially in the area that might have been neglected before. From the simple correlation plot, we found that variable $mix$ has strong positive correlation with $prbarr$. It can make sense because face-to-face crimes are more severe and usually have more police concentration and resources that lead to higher probability of arrest. However, it might not be very easy for non face-to-face crime, which is still the majority of crimes that happened, but actually seem to have pretty low arrest rate. This conclusion can be obtained by the fact that crime rate ($crmrte$) is negatively correlated with probability of arrest($prbarr$) while percentage of face-to-face crime ($mix$) is positively correlated with probability of arrest ($prbarr$). Therefore, how to increase $prbarr$ for non face-to-face crime is really the strategy we need to focus our energy on.

### Fitting model 1
__model 1__: $crmrte = \beta_0 + \beta_1 * prbarr + \beta_2 * prbconv + \beta_3 * avgsen + \beta_4 * prbpris + u $ <br>

Based on the understanding of what each parameter stands for, we have chosen crmrte as the only dependent variable to use for our analysis. The variable is a direct indicator of average crime commited to North Carolina counties. To start with, we took a look at the distribution of the dataset.<br>

From the analysis of the EDA, the distribution of crmrte does not look particularly normal, but the log transformed crmrte looks much more normally distributed. To reduce the standard error in the model building process, we decided that in our model fitting, we are going to use the log transformed $crmrte$ as our dependent variable. <br>
That being said, we are creating another variable that indicates the log transformed $crmrte$
```{r }
data_clean$crmrte_log <- log(data_clean$crmrte)
hist(data_clean$crmrte_log, breaks=50, main="log transformation of 
crime rate per capita")
```

Looking at the other variables, and the simple correlation plot in initial EDA, we are proposing the explanatory variables that we believe contribute to crime rate: <br>

1. prbarr: the probability of arrest should be a direct contributing factor to crime rate. In other words, if people who have potential to commit a crime believe the chance of them getting arrested is small, then it might encourage them to commit a crime
2. prbconv: after getting arrested, getting suspects convicted are the only way to let them take the punishment they deserve.
3. prbpris
4. avgsen: both probability of prison sentence reflect the severity of the punishment, which should directly impact the crime rate

With the above being proposed, we decided to take a look at the distribution of each explanatory variable:

```{r }
# prbarr
hist(data_clean$prbarr,breaks=50,main="prbarr", xlab="prbarr")
```

```{r }
#prbconv
hist(data_clean$prbconv,breaks=50,main="prbconv", xlab="prbconv")
```

```{r }
#prbpris
hist(data_clean$prbpris,breaks=50,main="prbpris", xlab="prbpris")
```

```{r }
#avgsen
hist(data_clean$avgsen,breaks=50,main="avgsen", xlab="avgsen")

```

The above histograms of each explanatory variable all seem rather normally distributed, with some anomalies implied in $prbarr$ and $avgsen$. However, we don't think there is enough evidence to make a decision on whether the data is anomaly or not, and we will use the data for model building.


Next, we want to look at the scatterplot between each key explanatory variable and the dependent variable to decide on model specification:

```{r }
scatterplot(data_clean$prbarr,data_clean$crmrte_log, main="prbarr vs. crmrte_log", xlab="prbarr", ylab="crmrte_log")
```
```{r }
scatterplot(data_clean$prbconv,data_clean$crmrte_log, main="prbconv vs. crmrte_log", xlab="prbconv", ylab="crmrte_log")
```
```{r }
scatterplot(data_clean$prbpris,data_clean$crmrte_log, main="prbpris vs. crmrte_log", xlab="prbpris", ylab="crmrte_log")
```
```{r }
scatterplot(data_clean$avgsen,data_clean$crmrte_log, main="avgsen vs. crmrte_log", xlab="avgsen", ylab="crmrte_log")
```

_Fitting model 1__:<br>
With the above explained reason, we are building the model1 with the explanatory variable of key interest as below:<br>
crmrte_log = $\beta_0 + \beta_1 * prbarr + \beta_2 * prbconv + \beta_3 * avgsen + \beta_4 * prbpris + u $ 

```{r }
model1 <- lm(crmrte_log ~ prbarr + prbconv + avgsen + prbpris, data=data_clean)
model1

```
Now let's look at whether the model coefficients satisfy the 6 CLM assumptions:

1. linear population model:<br>
\tab we have not defined the error term, therefore we don't have to talk about the linear population model yet.

2. Random sampling:<br>
\tab we do not know how the data is collected. But based on the source of the data, the probability of arrest, conviction and sentence should be random-sampled since they all come from credible sources that only collect data from actual events Therefore, we should be confident enough to say that the datset have randome sampling.

3. No perfect collinearity<br>
\tab We are relying on R to alert us when this happens

4. Zero-condition mean <br>
\tab Based on the chart below, it definitely deviates from zero-condition mean. It seems to be rather following a parabolic shape.<br>
\tab We believe it can be caused by both the face that:
\tab a. there are omitted variables 
\tab b. some parameters might have been misspecified. However, from the scatterplot, we cannot seem to fine strong evidence that the model is misspecified, therefore we are leaning towards the fact that there are other important variables that are omitted from the model
```{r}
plot(model1,which = 1)
```

5. Homoskedasticity <br>
\tab The below plot is not anywhere close to being flat. It suggests that there are heteroskedasticity in our model, therefore, to look at the errors for the model, we need to rely on the robust standard errors

```{r}
plot(model1,which = 3)

```

6. Normality of Errors <br>
\tab The Q-Q plot below suggests that most of the data points follow normal distribution relatively well, with some divergence towards the two ends. Since the two ends have fewer data points and we have close to 100 data points, we can take advantage of central limit theorem.

```{r}
plot(model1,which =2)
```

```{r}
plot(model1,which = 5)
```

The above plot shows that even though there are some points that have high leverage, but since it stays within the 0.5 Cook's distance, there is nothing that we need to worry about.

Next, we are doing a t-test for the coefficients to see whether they have statistical significance

```{r}
coeftest(model1,vcov=vcovHC)
```

Now we want to test whether the two explanatory variables that are not statistically significant are jointly significant, we build a model1.1 with just $prbarr$ and $prbconv$
```{r}
model1.1 <- lm(crmrte_log ~ prbarr + prbconv, data=data_clean)
```
To test whether the difference in fit is significant, we use the wald test, which generalizes the usual F-test of overall significance, but allows for a heteroskedasticity-robust covariance matrix
```{r}
waldtest(model1, model1.1, vcov = vcovHC)
```

The above results suggest that the variable $avgsen$ and $prbpris$ are not even jointly significant.


__Summary for model 1__ <br>
1. There are strong evidence that omitted variables exist for model1, and it leading to biases.
2. We need to use robust standard errors
3. Only $prbarr$ and $prbconv$ are statistically significant
4. $avgsen$ and $prbpris$ are not even jointly significant, therefore, we might need to consider removing those two variabls in our model


### Fitting model 2
Based on the analysis from research question section, we do believe that the variable $mix$ has certain influence on crime rate, especially in $prbarr$. The fact that $prbarr$ has strong correlation with $mix$ lead us to think that we should really be focusing on the portion of crime that is non face-to-face, which has relatively low $prbarr$. Therefore, we are adding an extra column that reflects percentage of non face-to-face crime: <br>

```{r}
hist(data_clean$mix,breaks=50,main="mix", xlab="mix")

```

The above histogram does not seem very normally distributed. Since it is left skewed, we believe that log transformation is helpful:
```{r}
hist(log(data_clean$mix),breaks=50,main="log transformed mix", xlab="log transformed mix")
```

We will create a new variable that has the log transformed $mix$, and name it $mix_log$.

```{r}
data_clean$mix_log <- log(data_clean$mix)
scatterplot(data_clean$mix_log,data_clean$crmrte_log, main="mix_log vs. crmrte_log", xlab="mix_log", ylab="crmrte_log")
```

1. mix: The above two scatterplots show enough evidence that variable $nonFtF$ is an important covariate. Logically, we believe that $nonFtF$ should directly impact $prbarr$ since the non face-to-face crime tend to be less severe, and there are not as much evidence that can lead to arrest. With the probability of arrest being low for this kind of crime, people are willing to take the risk to commit those crimes. Reducing the non face-to-face crime is very important in reducing the ocerall $crmrte$. Therefore, we believe that $nonFtF$ is crucial in model building and it will help make our model more accurate and less biased.

2. polpc: the magnitude of the $polpc$ is at least 100 times smaller than the other probability variables, and we believe that it might add some imbalance in our model building process, therefore we decided to create another variable that is 100 times of polpc:
```{r }
data_clean$polpcMult <- data_clean$polpc*100
hist(data_clean$polpcMult,breaks=50,main="polpcMult", xlab="polpcMult")
```

3. taxpc: 
```{r }
hist(data_clean$taxpc,breaks=50,main="taxpc", xlab="taxpc")
```

```{r }
hist(log(data_clean$taxpc),breaks=50,main="log transformed taxpc", xlab="log transformed taxpc")
```

The log transformed taxpc seems to be more normal compared to the original dataset. We will create a new variable with log transformed $taxpc$, and name it $taxpc_log$.
```{r}
data_clean$taxpc_log <- log(data_clean$taxpc)
```

4. urban: we will use $urban$ as indicator variable to see how we compare the data between areas outside the city and within the city

5. pctymle: According to the distribution below, there seems to be one point that has much higher percentage of young males. However, we don't have enough evidence to see whether it is an error in the data or an outlier.
```{r}
hist(data_clean$pctymle, breaks=50, main="pctymle", xlab="pctymle")
```
```{r}
hist(log(data_clean$pctymle), breaks=50, main="pctymle", xlab="pctymle")
```

With the above comparison, we will use the log transformed version of the data, and create a new variable $pctymle_log$
```{r}
data_clean$pctymle_log <- log(data_clean$pctymle)
```


_Fitting model 2__:<br>
With the second model, we are building it on top of model 1 but adding more variables that can have influence on the dependent variable crmte. After the data transformation, the model is as below: <br>
crmrte_log = $\beta_0 + \beta_1 * prbarr + \beta_2 * prbconv + \beta_3 * avgsen + \beta_4 * prbpris + \beta_5 * mix_log + \beta_6 * taxpc_log + \beta_7 * urban + \beta_8 * pctymle_log + \beta_9 * polpcMult + u $ 

```{r }
model2 <- lm(crmrte_log ~ prbarr + prbconv + avgsen + prbpris + mix_log + taxpc_log + pctymle_log + polpcMult, data=data_clean)

model2
```
Based on the model, we notice that nonFtf has a siginificant positive effect on crmrte_log. That means the higher the non face to face offense ratio, the higher the crmrte_log. This might be interprated as, non face to face offense tends to be more violent and dangerous than face to face offend, thus get reported more. Taxpc has almost none effect on crmrte. Urban has a relativly siginificant.

We will look again at the residuals and standard errors of the new model, and how is the model fitting satisfy the CLM assumptions.

1. Zero-condition mean <br>
\tab Based on the chart below, it definitely improves compared to model1, but still deviates from zero-condition mean. <br>
\tab Similarly, We believe it can be caused by both the face that:
\tab a. there are omitted variables 
\tab b. some parameters might have been misspecified. However, from the scatterplot, we cannot seem to fine strong evidence that the model is misspecified, therefore we are leaning towards the fact that there are other important variables that are omitted from the model
```{r}
plot(model2,which = 1)
```

2. Homoskedasticity <br>
\tab The below plot is not anywhere close to being flat. It suggests that there are heteroskedasticity in our model, therefore, to look at the errors for the model, we need to rely on the robust standard errors

```{r}
plot(model2,which = 3)

```

3. Normality of Errors <br>
\tab The Q-Q plot below suggests that most of the data points follow normal distribution relatively well, with some divergence towards the two ends. Since the two ends have fewer data points and we have close to 100 data points, we can take advantage of central limit theorem.

```{r}
plot(model2,which =2)
```

```{r}
plot(model2,which =5)
```

There is one point that is extremely close to the Cook's distance, but still within. We do not have to worry about that point with high leverage.

Next, we are doing a t-test for the coefficients to see whether they have statistical significance

```{r}
coeftest(model2,vcov=vcovHC)
```

__Summary for model 2__ <br>
1. $Prbarr$ remains to be statistically significant, but $prbconv$ has a lot of standard error introduced by adding more variables in, meaning that $prbconv$ is not the parameter to add in for a robust predictino model.
2. The addition of the other variables helped us in making the model more accurate. The adjusted r square is higher meaning that the new variables added to the model accuracy and the impact is more than by chance.
```{r}
paste("adjusted R square for model1 is:",summary(model1)$adj.r.squared,"; ", "adjusted R square for model2 is:",summary(model2)$adj.r.squared)
```

3. We want to test whether $mix_log$, $taxpc_log$, $pctymle_log$ and $polpcMult$ are jointly significant, meaning that whether their coefficients are all zero.
```{r}
linearHypothesis(model2, c("mix_log = 0", "taxpc_log = 0", "pctymle_log = 0", "polpcMult = 0"), vcov = vcovHC)
```

The output above tell us that the the 4 variables are jointly significant. There is a probability that there is multicollinearity among those four variables and which explains why none of them is individually significant.


### Fitting model 3
Bringing in other covariates that we do not believe to be too relavent:

```{r}
model3 <- lm(crmrte_log ~ prbarr+prbconv+prbpris+avgsen+polpcMult+density+taxpc_log+west+central+urban+pctmin80+wcon+wtuc+wtrd+wfir+wser+wmfg+wfed+wsta+wloc+mix_log+pctymle_log, data=data_clean)
#model3 <- lm(crmrte_log ~ prbarr+prbconv+prbpris+avgsen+polpcMult+density+taxpc+west+central+urban+pctmin80+mix_log+pctymle, data=data_clean)
model3
```

Next, we are interested in knowing how is model3 fitting into the CLM assumptions:<br>
4. Zero-condition mean <br>
\tab Based on the chart below, it definitely improves compared to model1, but still deviates from zero-condition mean. <br>
\tab Similarly, We believe it can be caused by both the face that:
\tab a. there are omitted variables 
\tab b. some parameters might have been misspecified. However, from the scatterplot, we cannot seem to fine strong evidence that the model is misspecified, therefore we are leaning towards the fact that there are other important variables that are omitted from the model
```{r}
plot(model3,which = 1)
```

5. Homoskedasticity <br>
\tab The below plot is not anywhere close to being flat. It suggests that there are heteroskedasticity in our model, therefore, to look at the errors for the model, we need to rely on the robust standard errors

```{r}
plot(model3,which = 3)

```

6. Normality of Errors <br>
\tab The Q-Q plot below suggests that most of the data points follow normal distribution relatively well, with some divergence towards the two ends. Since the two ends have fewer data points and we have close to 100 data points, we can take advantage of central limit theorem.

```{r}
plot(model3,which =2)
```

```{r}
plot(model3,which =5)
```

the above plot shows that we don't have to worry about the points with high leverage.

Next we want to check whether all variables related to wage are jointly significant. We built another model with all other variables that are not wage related, and want to use the wald test to check for statistical significance.
```{r}
model4 <- lm(crmrte_log ~ prbarr+prbconv+prbpris+avgsen+polpcMult+density+taxpc_log+west+central+urban+pctmin80+mix_log+pctymle_log, data=data_clean)
waldtest(model3, model4, vcov = vcovHC)

```

the above result tells us that those variables are jointly significant, and there is probability a great amount of multicollinearity that caused most of them to be none-statistically-significant. Also, wage factor and economics are widely studied factors related to crime, which is out of the scope of our research question. We just want to make sure that omitting wage factors does not introduce significant amount of biase.

Next we want to use Stargzer to directly compare the 4 models that we builts:
---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(stargazer)
```

Here is the latex table in a PDF document:

```{r mylatextable, results = "asis"}
stargazer(model1,model2,model3,model4, type = 'text', omit.stat = "f",star.cutoffs = c(0.05, 0.01, 0.001))

```
### What does the above table tell us:
1. model3 really help us to identify the most cost-effective measures that we should suggest in the policy to reduce crime rate. In the above table comparison, it is easy to see that the wage related variables that we intend to drop off from the analysis due to our research question, do have strong statistical significance yet the coefficients are small enough that we would not consider them to be the most important factors in our analysis. The addition of those variables does increase the adjusted R squre and the residuals plot shows more consistency with the CLM assumptions. However, the strong statistical significance + small coefficients serve as confirmation that those variables have small (close to zero) impact on the crime rate, which is not what we are trying to find in this research.

2. $prbarr$ is one of the most relevant factor in reducing crime rate. This is re-affirmed by comparing between different models. It is highly significant and the coefficient is high enough for us to consider it as an cost-effective measures to suggest policies for reducing crime rate

3. $polpcMult$ police per capita has high statistical significance, but we believe it does not show causality, or at least it does not have practical significance for us to suggest policies. With the background knowledge, we know that the police density will increase due to the high crime rate in a certain area, instead of the other way around. 


### Omitted Variable

1. __education level__: One aspect of education level that is measurable would be the years of education $year_educ$. <br>
The years of education could result in the biase over how much impact the percentage of young males $pctymle$ is correlated with $crmrte_log$. With the addition of education level, $pctymle$ should really become vague as a explanatory variable. In its place, there should be most likely $pctymle_lowEduc$ (percentage of low-education (<6 years) male) that is really more correlated. The bias from this omiitted variabled should be towards zero.

2. __surveillance camera__: Number of surveillance cameras $surCams$ has some impact on the prbability of arrest and the crime rate:
Leaving other variables unchanged, we assume that:
$crmrte_log = \beta_0 + \beta_1 * prbarr + \beta_2 * surCams + u $
$surCams = \gamma_0 + \gamma_1 * prbarr + v $

Based on our understanding, $\gamma_1$ should be positive, and $\beta_2$ should be negative. $\beta_1$ is negative. Therefore, the OLS coefficient of $prbarr$ will be scaled away to be more negative, gaining statistical significance.

3. __economic growth__: The annual growth in economic $ecoGrowth$ will lead to more job availability and increase in tax income, and will give more funding to the government to hire police forces.

$crmrte_log = \beta_0 + \beta_1 * taxpc + \beta_2 * ecoGrowth + u $
$ecoGrowth = \gamma_0 + \gamma_1 * taxpc + v $
Based on our understanding, $\gamma_1$ should be positive, if $\beta_2$ is positive, and $\beta_1$ is positive Therefore, the OLS coefficient of $prbarr$ will be scaled away to be more positive, gaining statistical significance.


4. __arrest rate for minor crimes__: the rate or probability of arrest for minor crimes (vandalism, public drinking, etc.) $prbarr_minor$ is the biggest percentage of overall crime rate, and it should be directly related to $prbarr$ and $crmrte_log$.

$crmrte_log = \beta_0 + \beta_1 * prbarr + \beta_2 * prbarr_minor + u $
$prbarr_minor = \gamma_0 + \gamma_1 * prbarr + v $
Based on our understanding, $\gamma_1$ should be positive, indicating that actually reducing overall crime rate will usually reduce crime rate for minor crime, and vice versa; $\beta_2$ should be negative so OMVB=\beta_2\gamma_1<0, and $\beta_1$ is negative Therefore, the OLS coefficient of $prbarr$ will be scaled away to be more negative, gaining statistical significance.


5. __average age__: the average age is affecting the crime rate because it is usually the group of people under 30 that commit more crime than people over 30:
$crmrte_log = \beta_0 + \beta_1 * pctymle + \beta_2 * avgAge + u $
$avgAge = \gamma_0 + \gamma_1 * pctymle + v $
$\gamma_1$ should be negative, and $\beta_2$ should be negative so OMVB=\beta_2\gamma_1>0, and $\beta_1$ is positive Therefore, the OLS coefficient of $pctymle$ will be scaled away to be more positive, gaining statistical significance.

6. __income inequality__: income alone is usually not enough to tell how it can be related to crime rate, we always need extra information like income growth and income inequality to analyze the social stability of the society. It is hard to say exactly what coefficients income inequality is going to affect, but it should be related to tax income, population density, what type of crime that happens, etc.


### Suggested Statistical Test:
1. Heavier penalties and arrest rate for minor crimes: the analysis suggest that less severe crimes consist of the most crimes commited, and the arrest rate is low for this type of crime. One good test in this regard is to lift the penalties and arrest rate for minor crimes for about 2 weeks, and see if the overall crime rate over those two weeks go down.

### Final conclusion:
Based on our models, we think that to reduce crime rate in general, there are severl approaches and can eventaully evolve into policies:

1. Increase the probability of arrest on non face to face crime. In order to achieve that, we need to furthur categorize non face to face crime and come up with strategies that can deal with each caregory. For example, most non face-to-face crimes are hard to identify and there are fewer evidences left. In order to catch more of those crimes in actions and with evidences, one policy that we will suggest is to install more surveillance cameras, especially in the areas where non face-to-faces crimes are most reported (downtowns, subways, malls, etc.)

2. If the above suggested statistical test has a positive result, we might need to consider increase penalties for minor crimes. Penalties for minor crime do not necessarily mean longer prison time. It could be in other forms like social services. 

3. Above are two policies on the measures we can take directly related to crime and arrest. However, in most of our omitted variables, if we have enough information about those variabels, there is a possiblity that we can draw conclusion from other aspects, for example, economically how to reduce income inequality. Collect more data on the omitted variables. We need further investigation after collecting information on the omitted variables. The models we have in this report all have their biases, so with more information, we can refine our models thus uncover the other key measures to reduce crime rate. 









